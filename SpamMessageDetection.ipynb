{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Pre-processing"
      ],
      "metadata": {
        "id": "f79qSVXo-eFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxv45maVpeRi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, Bidirectional\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/spam.csv', encoding='latin1')\n",
        "    df = df[['v1', 'v2']]\n",
        "    df = df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
        "    print(\"Data loaded successfully from spam.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: spam.csv not found.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize Data and Train-Test splits"
      ],
      "metadata": {
        "id": "jUYxtMFU-in3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_words = 1000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "max_len = 20\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "y_train = y_train.map({'ham': 0, 'spam': 1})\n",
        "y_test = y_test.map({'ham': 0, 'spam': 1})\n",
        "\n",
        "print(\"Shape of X_train_padded:\", X_train_padded.shape)\n",
        "print(\"Shape of X_test_padded:\", X_test_padded.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "xL_3HWPwphsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Glove and create Embedding Layer"
      ],
      "metadata": {
        "id": "4_UaGPoi-syG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "    embedding_dim = glove_model.vector_size\n",
        "    print(\"GloVe embeddings loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading GloVe embeddings: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(max_words, len(word_index) + 1)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < num_words:\n",
        "        try:\n",
        "            embedding_vector = glove_model[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                            input_length=max_len,\n",
        "                            trainable=False)\n"
      ],
      "metadata": {
        "id": "7LRulchSpkYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Model"
      ],
      "metadata": {
        "id": "cIW9nmzH-xd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "bT3FL0vgsP9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "aJkVY89u-1Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(X_train_padded, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "wUmmAOJ_sUyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results and Evaluations"
      ],
      "metadata": {
        "id": "-vnxu02A_AIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test_padded)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "70IuuYAysbtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}